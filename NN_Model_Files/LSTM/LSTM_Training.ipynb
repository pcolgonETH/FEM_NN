{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import libraries\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import json\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_data_with_columns(input_path, num_simulations, num_increments, num_features):\n",
    "    \"\"\"\n",
    "    Load data while ignoring the first four columns, then reshape for LSTM input.\n",
    "\n",
    "    Args:\n",
    "    - input_path (str): Path to the data file.\n",
    "    - num_simulations (int): Number of simulations in the dataset.\n",
    "    - num_increments (int): Number of increments per simulation.\n",
    "    - num_features (int): Number of relevant features after.\n",
    "\n",
    "    Returns:\n",
    "    - reshaped_data (np.ndarray): Data reshaped to (num_simulations, num_increments, num_features).\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(input_path, header=None)\n",
    "\n",
    "    data = data.iloc[:, 0:num_features]\n",
    "    \n",
    "    # Convert to numpy and reshape\n",
    "    data_np = data.to_numpy()\n",
    "    reshaped_data = data_np.reshape(num_simulations, num_increments, num_features)\n",
    "\n",
    "    return reshaped_data"
   ],
   "id": "ce44a3cfdf7bb731",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_path = 'path_to_input_file'\n",
    "target_path = 'path_to_target_file'"
   ],
   "id": "78d945988dd63ff7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define parameters\n",
    "num_simulations, num_increments, num_features, num_targets = 863, 1000, 12, 6\n",
    "feature_data = load_data_with_columns(input_path, num_simulations, num_increments, num_features)\n",
    "target_data = load_data_with_columns(target_path, num_simulations, num_increments, num_targets)"
   ],
   "id": "3183cbef2c221bdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def manual_minmax_scaler(data, feature_range=(-1, 1)):\n",
    "    \"\"\"\n",
    "    Min-max scaler that computes data_min, data_max, and scale for each feature or target independently.\n",
    "    \"\"\"\n",
    "    # Compute min and max for each feature/target across all simulations and time steps\n",
    "    data_min = np.min(data, axis=(0, 1))  # Minimum for each feature/target\n",
    "    data_max = np.max(data, axis=(0, 1))  # Maximum for each feature/target\n",
    "    data_range = data_max - data_min\n",
    "    scale = np.where(data_range != 0, (feature_range[1] - feature_range[0]) / data_range, 0)\n",
    "\n",
    "    # Scale data for each feature/target\n",
    "    data_scaled = np.where(data_range != 0, scale * (data - data_min) + feature_range[0], feature_range[0])\n",
    "    return data_scaled, data_min, data_max, scale\n",
    "\n",
    "def manual_inverse_transform(scaled_data, data_min, data_max, scale, feature_range=(-1, 1)):\n",
    "    return np.where(scale != 0, (scaled_data - feature_range[0]) / scale + data_min, data_min)"
   ],
   "id": "261b4ffc9da7dd73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_data(data, scaler_function=manual_minmax_scaler):\n",
    "    \"\"\"\n",
    "    Normalize data for each feature/target independently.\n",
    "    \"\"\"\n",
    "    normalized_data, data_min, data_max, scale = scaler_function(data)\n",
    "    scaling_params = {'data_min': data_min, 'data_max': data_max, 'scale': scale}\n",
    "    return normalized_data, scaling_params"
   ],
   "id": "393e4f0d67ebf166",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "feature_data_normalized, feature_scaler = normalize_data(feature_data)\n",
    "target_data_normalized, target_scaler = normalize_data(target_data)"
   ],
   "id": "7ec34c3975fc08bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(feature_scaler)",
   "id": "f8c310b7f45df72a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Gates: input, forget, cell, and output\n",
    "        self.W_i = nn.Linear(input_size, hidden_size)\n",
    "        self.U_i = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_f = nn.Linear(input_size, hidden_size)\n",
    "        self.U_f = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_c = nn.Linear(input_size, hidden_size)\n",
    "        self.U_c = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_o = nn.Linear(input_size, hidden_size)\n",
    "        self.U_o = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        # Input gate\n",
    "        i_t = torch.sigmoid(self.W_i(x) + self.U_i(h_prev))\n",
    "        #print(i_t.shape)\n",
    "        # Forget gate\n",
    "        f_t = torch.sigmoid(self.W_f(x) + self.U_f(h_prev))\n",
    "        #print(f_t.shape)\n",
    "        # Cell candidate\n",
    "        c_hat_t = torch.tanh(self.W_c(x) + self.U_c(h_prev))\n",
    "        #print(c_hat_t.shape)\n",
    "        # Cell state\n",
    "        c_t = f_t * c_prev + i_t * c_hat_t\n",
    "        #print(c_t.shape)\n",
    "        # Output gate\n",
    "        o_t = torch.sigmoid(self.W_o(x) + self.U_o(h_prev))\n",
    "        #print(o_t.shape)\n",
    "        # Hidden state\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        #print(h_t.shape)\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim  # Can be an int or a list\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        if isinstance(hidden_dim, int):\n",
    "            self.hidden_dim = [hidden_dim] * num_layers\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList([\n",
    "            CustomLSTMCell(\n",
    "                input_size=input_dim if i == 0 else self.hidden_dim[i - 1],\n",
    "                hidden_size=self.hidden_dim[i]\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_dim[-1], output_dim)\n",
    "\n",
    "    def forward(self, x_t, h_t, c_t):\n",
    "        \"\"\"Process a single timestep.\"\"\"\n",
    "        for i, lstm_cell in enumerate(self.lstm_cells):\n",
    "            if i == 0:\n",
    "                h_t[i], c_t[i] = lstm_cell(x_t, h_t[i], c_t[i])\n",
    "            else:\n",
    "                h_t[i], c_t[i] = lstm_cell(h_t[i - 1], h_t[i], c_t[i])\n",
    "        output = self.fc(h_t[-1])\n",
    "        return output, h_t, c_t"
   ],
   "id": "ec3e980fc639032d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_features, test_features, train_targets, test_targets = train_test_split(\n",
    "    feature_data_normalized, target_data_normalized, test_size=0.1, random_state=42)\n",
    "batch_size = 100\n",
    "train_dataset = TensorDataset(torch.Tensor(train_features), torch.Tensor(train_targets))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(torch.Tensor(test_features), torch.Tensor(test_targets))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "9b8e9a262fbcfec1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_custom_lstm_timestep(model, train_loader, num_epochs, learning_rate,loss_save_path):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    epoch_losses = []  # List to store epoch losses\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            batch_size, seq_len, _ = inputs.size()\n",
    "            h_t = [torch.zeros(batch_size, model.hidden_dim[i], device=device)\n",
    "                   for i in range(model.num_layers)]\n",
    "            c_t = [torch.zeros(batch_size, model.hidden_dim[i], device=device)\n",
    "                   for i in range(model.num_layers)]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = []\n",
    "\n",
    "            # Process each timestep\n",
    "            for t in range(seq_len):\n",
    "                x_t = inputs[:, t, :]\n",
    "                y_t = targets[:, t, :]\n",
    "                output_t, h_t, c_t = model.forward(x_t, h_t, c_t)\n",
    "                outputs.append(output_t.unsqueeze(1))\n",
    "\n",
    "            outputs = torch.cat(outputs, dim=1)  # Shape: (batch_size, seq_len, output_dim)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_losses.append(epoch_loss)  # Save epoch loss\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.8f}\")\n",
    "\n",
    "    # Save epoch losses to a text file\n",
    "    with open(loss_save_path, 'w') as f:\n",
    "        for epoch, loss in enumerate(epoch_losses, 1):\n",
    "            f.write(f\"Epoch {epoch}: {loss:.8f}\\n\")\n",
    "\n",
    "    print(f\"Losses saved to {loss_save_path}\")\n",
    "\n",
    "    return model"
   ],
   "id": "cc63318247cdf7ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate the model\n",
    "model = LSTMModel(input_dim=12, hidden_dim=50, num_layers=2, output_dim=6)\n",
    "# Train the model\n",
    "loss_save_path = 'path_to_loss_file'\n",
    "model = train_custom_lstm_timestep(model, train_loader, num_epochs=2, learning_rate=0.0002, loss_save_path=loss_save_path)"
   ],
   "id": "1270adaffa0c3259",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_path = 'path_to_save_model'\n",
    "save_path_pt = 'path_to_save_scripted_model'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(save_path_pt)"
   ],
   "id": "eb3eea95df02b127",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save function for scaling parameters\n",
    "def save_scaling_params(scaler, filepath):\n",
    "    \"\"\"\n",
    "    Save scaling parameters to a JSON file.\n",
    "\n",
    "    Args:\n",
    "    - scaler (dict): Dictionary with scaling parameters.\n",
    "    - filepath (str): Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to lists\n",
    "    scaler = {key: value.tolist() if isinstance(value, np.ndarray) else value\n",
    "                           for key, value in scaler.items()}\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(scaler, f)\n",
    "\n",
    "# Save feature and target scalers\n",
    "save_scaling_params(feature_scaler, 'path_to_feature_scaler')\n",
    "save_scaling_params(target_scaler, 'path_to_target_scaler')"
   ],
   "id": "44fb2447f183822c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to convert JSON structure to arrays\n",
    "def extract_scaling_params(json_file):\n",
    "    \"\"\"\n",
    "    Extract scaling parameters from a JSON file.\n",
    "\n",
    "    Args:\n",
    "    - json_file (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - min_vals (np.ndarray): Array of minimum values for each feature/target.\n",
    "    - max_vals (np.ndarray): Array of maximum values for each feature/target.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\") as f:\n",
    "        scaling_params = json.load(f)\n",
    "    min_vals = np.array(scaling_params['data_min'])\n",
    "    max_vals = np.array(scaling_params['data_max'])\n",
    "    return min_vals, max_vals\n",
    "\n",
    "# Function to save arrays with comma-separated formatting, ensuring no line breaks\n",
    "def save_array_with_commas(file_path, array):\n",
    "    \"\"\"\n",
    "    Save an array to a file as a single comma-separated line.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the output text file.\n",
    "    - array (np.ndarray): Array to save.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        # Write all elements in the array as a single comma-separated line\n",
    "        array_str = \",\".join(f\"{val:.8e}\" for val in array)\n",
    "        f.write(array_str + \"\\n\")\n",
    "\n",
    "# Process feature scaling parameters\n",
    "feature_min_vals, feature_max_vals = extract_scaling_params(\n",
    "    \"path_to_feature_scaler\"\n",
    ")\n",
    "save_array_with_commas(\n",
    "    \"path_to_feature_min_vals\",\n",
    "    feature_min_vals,\n",
    ")\n",
    "save_array_with_commas(\n",
    "    \"path_to_feature_max_vals\",\n",
    "    feature_max_vals,\n",
    ")\n",
    "\n",
    "# Process target scaling parameters\n",
    "target_min_vals, target_max_vals = extract_scaling_params(\n",
    "    \"path_to_target_scaler\"\n",
    ")\n",
    "save_array_with_commas(\n",
    "    \"path_to_target_min_vals\",\n",
    "    target_min_vals,\n",
    ")\n",
    "save_array_with_commas(\n",
    "    \"path_to_target_max_vals\",\n",
    "    target_max_vals,\n",
    ")\n",
    "\n",
    "print(\"Scaling parameters saved as text files with commas, ensuring no line breaks.\")\n"
   ],
   "id": "1b6a7d217fa89a99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def validate_and_plot_single_simulation(\n",
    "    model, val_loader, target_scaler, timestep_mode=True, save_directory=\".\"\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    h_t_list = []\n",
    "    c_t_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in val_loader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            batch_size, seq_len, input_dim = inputs.size()\n",
    "\n",
    "            if timestep_mode:\n",
    "                h_t = [torch.zeros(batch_size, model.hidden_dim[layer], device=device)\n",
    "                       for layer in range(model.num_layers)]\n",
    "                c_t = [torch.zeros(batch_size, model.hidden_dim[layer], device=device)\n",
    "                       for layer in range(model.num_layers)]\n",
    "                timestep_outputs = []\n",
    "\n",
    "                for t in range(seq_len):\n",
    "                    x_t = inputs[:, t, :]\n",
    "                    output_t, h_t, c_t = model.forward(x_t, h_t, c_t)\n",
    "                    timestep_outputs.append(output_t.unsqueeze(1))\n",
    "\n",
    "                    h_t_list.append([h.detach().cpu().numpy() for h in h_t])\n",
    "                    c_t_list.append([c.detach().cpu().numpy() for c in c_t])\n",
    "\n",
    "                outputs = torch.cat(timestep_outputs, dim=1)\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            targets.append(target.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    targets = np.concatenate(targets, axis=0)\n",
    "    \n",
    "    data_min = target_scaler['data_min']\n",
    "    data_max = target_scaler['data_max']\n",
    "    scale = target_scaler['scale']\n",
    "    \n",
    "    # Inverse scaling for predictions and targets\n",
    "    predictions_original = manual_inverse_transform(predictions, data_min, data_max, scale, feature_range=(-1, 1))\n",
    "    targets_original = manual_inverse_transform(targets, data_min, data_max, scale, feature_range=(-1, 1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Scale only the first 3 features/targets\n",
    "    #columns_to_scale = [0, 1, 2]\n",
    "    #predictions_original = np.copy(predictions)\n",
    "    #targets_original = np.copy(targets)\n",
    "\n",
    "    #for col in columns_to_scale:\n",
    "        #predictions_original[:, :, col] = manual_inverse_transform(\n",
    "            #predictions[:, :, col], data_min[col], data_max[col], scale[col]\n",
    "        #)\n",
    "        #targets_original[:, :, col] = manual_inverse_transform(\n",
    "            #targets[:, :, col], data_min[col], data_max[col], scale[col]\n",
    "        #)\n",
    "\n",
    "    # Random simulation index for plotting\n",
    "    random_simulation_index = random.randint(0, predictions_original.shape[0] - 1)\n",
    "    num_outputs = predictions_original.shape[2]\n",
    "    time_increments = np.arange(predictions_original.shape[1])\n",
    "\n",
    "    # Create folder for plots\n",
    "    simulation_folder = os.path.join(save_directory, f\"Simulation_V1_{random_simulation_index}\")\n",
    "    os.makedirs(simulation_folder, exist_ok=True)\n",
    "    print(f\"Saving plots to folder: {simulation_folder}\")\n",
    "\n",
    "    for i in range(num_outputs):\n",
    "        plt.figure()\n",
    "        plt.plot(time_increments, predictions_original[random_simulation_index, :, i],\n",
    "                 label=\"Predicted\", linestyle=\"-\", marker=\"o\")\n",
    "        plt.plot(time_increments, targets_original[random_simulation_index, :, i],\n",
    "                 label=\"True\", linestyle=\"--\", marker=\"x\")\n",
    "        plt.xlabel(\"Time Increments\")\n",
    "        plt.ylabel(f\"Output {i + 1}\")\n",
    "        plt.title(f\"Predicted vs True for Output {i + 1} - Simulation {random_simulation_index}\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.gca().patch.set_facecolor('white')  # Set plot area background to white\n",
    "        plt.gca().patch.set_edgecolor('black')  # Set edges of the plot area to black\n",
    "        plt.gca().patch.set_linewidth(1)        # Set line width for the edges\n",
    "        # Remove inner edge spines (contour inside the plot area)\n",
    "        ax = plt.gca()\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "    \n",
    "        # Save the plot and confirm\n",
    "        plot_path = os.path.join(simulation_folder, f\"output_{i + 1}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved plot: {plot_path}\")\n",
    "\n",
    "    print(f\"All plots saved successfully in: {simulation_folder}\")\n",
    "    \n",
    "    print(targets_original[random_simulation_index, :, :])\n",
    "    \n",
    "    if timestep_mode:\n",
    "        return predictions_original, targets_original, h_t_list, c_t_list\n",
    "    else:\n",
    "        return predictions_original, targets_original"
   ],
   "id": "722260ad57e032bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = LSTMModel(input_dim=12, hidden_dim=50, num_layers=2, output_dim=6)\n",
    "model.load_state_dict(torch.load('path_to_save_model'))\n",
    "model.eval()\n",
    "validate_and_plot_single_simulation(model, val_loader, target_scaler, timestep_mode=True, save_directory='path_to_save_plots')"
   ],
   "id": "b4b2edd5f4c11bf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2cbf982d11faa4f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
